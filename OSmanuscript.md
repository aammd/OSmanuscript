**The Tao of Open Science for Ecology**

> Stephanie E. Hampton, Center for Environmental Research, Education and
> Outreach, Washington State University, Pullman WA 99164
>  s.hampton@wsu.edu
>
> Sean S. Anderson, Environmental Science and Resource Management
> Program & Pacific Institute for Restoration Ecology, California State
> University Channel Islands, Camarillo, CA 93012
>  sean.anderson@csuci.edu
>
> Sarah C. Bagby, Marine Science Institute and Department of Earth
> Science, University of California, Santa Barbara, 93106
>  bagby@geol.ucsb.edu
>
> Xueying “Shirley” Han: xueyinghan85@gmail.com, Center for
> Nanotechnology in Society, University of California, Santa Barbara, CA
> 93106
>
> Edmund M. Hart, National Ecological Observatory Network, 1685 38th
> St., Suite 100, Boulder, CO 80301 edmund.m.hart@gmail.com
>
> Matthew B. Jones, National Center for Ecological Analysis and
> Synthesis, University of California, Santa Barbara, 735 State St.
> Suite 300, Santa Barbara, CA 93101 jones@nceas.ucsb.edu
>
> W. Christopher Lenhardt: clenhardt@renci.org, Renaissance Computing
> Institute (RENCI), University of North Carolina at Chapel Hill.
>
> Andrew MacDonald
> [*macdonald@zoology.ubc.ca*](mailto:macdonald@zoology.ubc.ca),
> Department of Zoology, University of British Columbia, Canada
>
> William K. Michener, College of University Libraries and Learning
> Science, MSC05 3020, University of New Mexico, Albuquerque, NM  87131
>  william.michener@gmail.com
>
> Joe Mudge: joe.mudge@ttu.edu, The Institute of Environmental and Human
> Health, Department of Environmental Toxicology, Texas Tech University,
> Lubbock, TX 79416
>
> Afshin Pourmokhtarian: apourmok@bu.edu   Department of Earth &
> Environment, Rm 130, 685 Commonwealth Avenue, Boston University,
> Boston, MA 02215
>
> Mark Schildhauer, National Center for Ecological Analysis and
> Synthesis, University of California, Santa Barbara 735 State St. Suite
> 300, Santa Barbara, CA 93101 schild@nceas.ucsb.edu
>
> Kara Woo, National Center for Ecological Analysis and Synthesis,
> University of California, Santa Barbara, 735 State St. Suite 300,
> Santa Barbara, CA 93101 woo.kara@gmail.com
>
> Naupaka Zimmerman naupaka@gmail.com School of Plant Sciences,
> University of Arizona, 1145 E 4th St., Tucson, AZ 85721
>
> **Corresponding author:** Stephanie Hampton, s.hampton@wsu.edu

**Abstract**

The field of ecology is poised to take advantage of emerging
technologies that facilitate the gathering, analyzing, and sharing of
data, methods, and results. The concept of transparency at all stages of
the research process, coupled with free and open access to data, code,
and papers, constitutes "open science". Despite the many benefits of an
open approach to science, a number of barriers to entry exist that may
prevent researchers from embracing openness in their own work. Here we
describe several key shifts in mindset that underpin the transition to
more open science. These shifts in mindset include thinking about data
stewardship rather than data ownership, embracing transparency
throughout the data life-cycle, and accepting critique in public. Though
foreign and perhaps frightening at first, these changes in thinking
stand to benefit the field of ecology by fostering collegiality and
broadening access to data and findings. We present an overview of tools
and best practices that can enable these shifts in mindset at each stage
of the research process, including tools to support data management
planning and reproducible analyses, strategies for soliciting
constructive feedback throughout the research process, and methods of
broadening access to final research products.

**Keywords:** data management; ecology; open access; open science;
reproducible research

**Introduction**

Ecology stands at the threshold of a potentially profound change. The
combination of ever-increasing computational power, coupled with
advances in Internet technologies and tools, is catalyzing new ways of
pursuing ecological investigations. These emerging approaches facilitate
greater communication, cooperation, collaboration, and sharing, not only
of results, but also of data, analytical and modeling code, and
potentially even fully documented workflows of the processes---warts and
all---that lead to scientific insights. This vision of free and
unfettered access to all stages of the scientific endeavor has been
called "open science" (Nielsen 2011). As an integrative and highly
multidisciplinary field, ecology particularly stands to benefit from
this open science revolution, and many ecologists have expressed
interest in enhancing the openness of ecology. To date, such
conversations among ecologists have largely occurred online (e.g.,
discussed in Darling et al 2013); thus it seems timely to present an
introduction of open science for ecologists who may or may not currently
be active in social media where the discussion is evolving. We give an
overview of the rise of open science, the changes in mindset that open
science requires, and the digital tools that can enable ecologists to
put the open science mindset into practice.

The exchange of scientific information was first institutionalized in
the 1660s with the establishment of the Philosophical Transactions of
the Royal Society of London and the Journal des Sçavans, the first
scientific journals (Beaver and Rosen 1978).  While these journals
provided platforms for scientists to share their results and ideas, they
were largely accessible only to elites---those who could afford a
subscription themselves, or those who belonged to an institution that
held copies (Nielsen 2011).  Scientists published in these journals to
establish precedence of discovery; the notion of collaboration among
scientists does not seem to have taken hold until the 1800s (Beaver and
Rosen 1978).

The scientific world looks very different now. Advances in computing
accelerated not only individual scientists’ discoveries but also their
collaborative potential (Box 1). Modern scientists constitute a new
invisible college with global reach, its philosophical transactions
enabled by the Internet (Wagner 2008).  Collaboration has become the
predominant norm for high-impact research (Wuchty et al 2007).
Technological developments also have enabled the capture of a previously
unimaginable volume of data and metadata at ever increasing rates
(Reichman et al 2011, Dietze, M.C., D. LeBauer, R. Kooper. 2013), and
the deployment of greater computational power to run models and analyze
data. Traditional paper notebooks cannot meet the challenges of
increased accumulation, sharing, and recombination of ideas, research
logs, data sets and analyses (Strasser and Hampton 2012). Nor can a
non-networked computer.  The tools and approaches that together
constitute open science can help ecologists to meet these challenges, by
amplifying opportunities for collaboration and demanding consistent
machine-readable documentation necessary for rapid reproducibility in
complex projects.

While interest in this new paradigm is on the rise (Fig. 1), it must be
acknowledged that both technical and sociocultural obstacles impede
adoption for some ecologists. For example, precedence, attribution,
investment, and payoff are high-stakes issues for professional
scientists (Hackett 2005). Adopting open practices means ceding some
control of these issues, and devoting precious time to learning new
modes of research and communication in a seemingly foreign language (Box
2). Yet hewing to traditional practices carries its own risks for the
individual investigators. Errors and oversights can persist far longer
when experimental design and data analysis are held in private; weeks
and months can be wasted in chasing reproduction of results because
methods are documented only as fully as a journal word count permits;
labs can become isolated, their advancement slowed, for lack of
substantive interaction with others. Open science can help to mitigate
these risks, to the immediate benefit of the individual practitioner, as
she builds an active community around her.

Moreover, open science promises many longer term benefits to the
scientific community. The adoption of standard best practices and
cultural norms for public archiving of data and code will speed
discovery and promote fairness in attribution. The use of open-source
tools and open-access data and journals will help to further democratize
science, diversifying perspectives and knowledge by promoting broader
access for scientists in developing countries and at under-resourced
institutions, and fostering citizen science which is already a major
source of data in some ecological sub-disciplines (Cooper 2014).

Here, we discuss the changes in mindset and the tools that can help
interested ecologists move toward practicing open science themselves,
facilitate its practice by their students and other colleagues, or both.

**Changes in mindset**

*Data stewardship, not data ownership*

Traditional views on data ownership hold that data are proprietary
products of the researcher (Sieber 1989). By definition, this data
ownership mindset limits the potential for data sharing as a given
researcher can restrict the conditions and circumstances by which their
data are disseminated. These views have persisted for a variety of
reasons (Sieber 1989, Hampton et al 2013, Lindenmeyer and Likens 2013)
and ecologists historically have treated data as proprietary whether or
not the data collection has been funded by taxpayers and might
reasonably be considered public property (Obama 2013).

Under the principles of open science, data are generated with the
expectation of unfettered public dissemination. This fundamental shift
in thinking from "I own the data" to "I collect and share the data on
behalf of the scientific community" is essential to the transparency and
reproducibility of the open science framework. When data are available,
discoverable, and well-described, scientists can avoid “reinventing the
wheel” and instead build directly on those products to innovate. For
example, authors’ reluctance to submit null results for publication
leads to a "file-drawer" effect that can not only systematically bias
the published literature (Iyengar and Greenhouse 1988, Franco et al.
2014) but also allows independent scientists to go repeatedly down the
same blind alleys. Structures to store, share, and integrate data
contribute to avoiding such waste of scientific resources. Beyond this
greater efficiency, data sharing also contributes to the production of
entirely new scientific products that were not envisioned at the time
data were collected (Carpenter et al 2009).

Norms have yet to be established in ecology for how soon after
collection data should be shared in order to promote openness and a
healthy scientific culture, and a range of data sharing practices is
currently employed by scientists who are philosophically aligned with
open science (Figure 2). A full embrace of open science data would mean
sharing data instantaneously, or upon completion of initial quality
assurance checks or other pre-processing (e.g., NEON; ref). In other
cases, researchers have made an argument for a constrained period of
exclusive access by researchers directly involved in data collection
(e.g. Sloan Digital Sky Survey; \<http://www.sdss.org/\>). In any case,
it is increasingly recognized in the requirements of funding agencies
that full data sharing in established repositories should begin no later
than the publication of results.

*Transparency throughout the data life-cycle*

Scientists publish their methodology with reproducibility by others in
mind, but have traditionally had to judge which details were important
to transmit within limitations imposed by print journals. The
availability of online supplementary methods sections gives scientists
scope to detail their methods more fully, and a broader suite of online
tools now creates opportunity to share the code, data, and detailed
decision making. Taking advantage of these opportunities to make tacit
knowledge explicit to others is a crucial part of performing
reproducible research (Collins 2001, Ellison 2010), and has the
substantial additional benefit of making such knowledge explicit to
oneself, with the potential to shed light on untested assumptions and
unidentified confounding effects.

Workflow tools (Table 1) now make it possible for scientists to make
every stage of the research process transparent, from sharing the
detailed rationale for an approach to publishing the data and code that
generated analyses and figures.  Detailed sharing of methods and code
improves clarity; personal communications regarding methods crucially
improves trust (Collins 2001), and social media permit these
communications too to happen in the open.  Openness throughout the data
life-cycle also provides the scientist with the opportunity to receive
extensive feedback from the rest of the scientific community and can
improve the quality of that feedback (Byrnes et al 2014).  Whereas
formal peer review provides feedback only at the project’s proposal
phase (for those seeking grant support) and conclusion, open science
provides an avenue for scientists to receive feedback at key junctures,
e.g., before experiments are performed.

Additionally, transparency encourages researchers to converge on
standard structures for data and code archiving (Table 1).  Such
convergence is particularly important for interdisciplinary science, in
which the fragmentation of resources and practices along disciplinary
boundaries can substantially hinder research.  Common standards and a
shared, searchable infrastructure help make data sets not merely open
but also discoverable, improving their reach and impact and helping
scientists identify potential new collaborators.

Having said all this, scientists need not fear that open science is only
for the exhibitionists among us; we recognize that there are many points
in the scientific process when deep, sometimes solitary reflection is
invigorating and productive.

*Acceptance of critique*

Failure is recognized as a normal and necessary part of the scientific
process, and yet academic science is structured to reward only being
right in public (Merton 1957), creating tension in practicing open
science. The more open our science, the greater the chance that our
mistakes as well as our insights will be publicly available. This
prospect can be frightening to contemplate; one study of physicists
found that those practicing secrecy prior to publication often did so to
avoid the risk of looking foolish (Gaston 1971).  We suggest that
embracing this tension gives us the opportunity to be better and more
productive scientists. The only way to protect our ideas and methods
from criticism indefinitely is to refrain from publication, hardly a
desirable outcome.  Even delaying exposure until peer review manages
only to limit the possible range of feedback to being told what could
have been done better. By contrast, adopting open practices throughout
the scientific endeavor makes it possible to receive and incorporate
critiques before our research products are complete. That is, by risking
the possibility of being briefly wrong in public, we improve our chances
of being lastingly, usefully right.

**Tools and best practices to enable shifts in mindset and practice**

An open science mindset affects the entire scientific process, carrying
responsibilities and offering benefits at each stage along the way
(Figure 2). If we are committed to data stewardship, planning an
experiment entails not only thinking through the physical manipulations
involved but also working out how to capture and share the data and
metadata that will enable others to effectively re-use that information.
 The open-source DMPTool (Table 1) offers guidance to scientists
creating data management plans---now often a prerequisite for
funding---and helps scientists find institutional resources for
implementation.  At the same time, ready access to data sets collected
by other scientists can help focus our questions, by identifying gaps
and opportunities, and improve our ability to answer them (e.g., by
allowing us to estimate and plan for experimental uncertainties). Once
data has been collected, the open scientist prepares the data set for
use by others and documents its provenance (e.g., with tools from
rOpenSci), then deposits it in a community-endorsed repository (e.g.,
Knowledge Network for Biocomplexity, Dryad).  This process ensures that
the data will remain usable and accessible for years to come, allowing
our work to be integrated into the body of knowledge and ensuring that,
when we return to a project after a period away, we can pick up where we
left off.

If we are committed to transparency, we document and share as much
information about the process as feasible.  Electronic lab notebooks
(e.g., using IPython notebooks) help track and share the reasoning
behind our experimental and analytical decisions, as well as the final
protocol and any deviations, and can be linked to the resulting data
files to keep research organized.  Adhering to the discipline of
consistently, carefully, and thoroughly documenting the research process
is an exercise in critical thinking, a constant reminder to check our
assumptions and clarify our thinking.  During data analysis,
reproducible, script-based methods (e.g., in R or Python) can be used
for every step from importing raw, uncleaned data to analysis and
production of figures and final manuscripts (e.g., FitzJohn et al 2014).
Such tools are essentially self-documenting along the way, providing a
complete record of data manipulations that is much more difficult to
generate for point-and-click analyses in a graphical user interface
(GUI).  While errors can be made in both scripted and GUI-based
analyses, the existence of a record makes errors in the former far
easier to detect and correct, protecting the integrity of the analysis.
Tools such as markdown and knitr facilitate integration of data analysis
into manuscript production, making it easier to keep figures and
reported results current as an analysis is refined.  All of these steps
are undertaken with an eye to making our work reproducible and open to
others, but all offer the immediate benefit of making our work
reproducible and open to ourselves. Many of the tools mentioned in Table
1 have proprietary analogs (e.g. as SAS is to R), and afford many
similar advantages, but exclusive use of open-source, free software
maximizes access by other researchers.

If we are committed to openness to critique, we make documentation
public as early as possible in the development of a project (Figure 2).
 Experiments can be discussed using the features of publicly available
electronic lab notebooks or on social media (Gewin 2013, Darling et al
2013); code can be examined and potentially improved in public code
repositories (e.g. Github); figures and movies can be opened for comment
on public websites (e.g. Figshare). In all cases, the open scientist
seeks repositories with stable identifiers (e.g., stable URLs and DOIs)
rather than relying more than necessary on one’s own servers. All of
these tools give us access to a research group far bigger than a single
lab, helping experimental designs to be improved and stimulating
discussion of worthwhile new directions, connections, and approaches.
 As a project draws to a close, preprints can be posted for comment from
a broader audience than a journal's handful of peer reviewers; preprints
also improve a project's visibility and, with the addition of a date
stamp, establish precedence (Desjardins-Proulx et al 2013).  Publishing
final papers in open-access journals ("gold" open access) or
self-archiving manuscripts ("green" open access) makes the final
products available to a wide audience, including the taxpayers who
funded the research.

Version control (e.g., Git, SVN) at every stage is a highly recommended
best practice (Noble 2009, Wilson et al 2014), supporting every aspect
of the open science mindset. Version control systems, accessible through
user-friendly tools like GitHub, allow scientists to retain snapshots of
previous analyses for future reference, collaborate easily and track
contributions, record ideas, and safeguard against the loss of code and
data (Ram 2013), thus preserving the long-term integrity of the project
even as collaborations form and shift.

**Conclusions**

Online tools make possible a future in which not only scientific
practice but also scientific culture are transformed by openness (Hey et
al 2009, Nielsen 2011).  Fully open science would be apparent from end
to end of discovery, from the sharing of nascent ideas that can be
improved upon by active discussion, to the instantaneous upload of data
at the moment of capture, through the full development of "living
papers" in an open forum in which the details of analysis and reasoning
are completely transparent.  Subsequent generations of ecologists will
build their work on what we leave.  If instead of paywalled journal
archives we leave them open-access repositories of data, code, and
papers, they will be far better equipped to push new frontiers in
science and create solutions to pressing societal problems.

Very real technological and cultural hurdles still stand between us and
this future:  investigators must be willing to invest time in learning
the tools that facilitate open science, and in re-learning them as the
tools evolve, while the scientific community must collectively establish
new norms for collegiality and reproducibility in the digital age.
Nevertheless, we can all move our research toward this future by
adopting the aspects of open science that are currently feasible for our
labs (e.g., publishing open-access articles; sharing all data and code
used in publications) and by supporting our students and junior
colleagues in developing the skills that will best prepare them for the
responsibilities, opportunities, and rewards of practicing ecology in an
open environment.

**Acknowledgments**

Much of the work on this manuscript was done at the Open Science
Codefest which was supported by NSF Grant Numbers 1216894 and 1216817
and sponsored by the Renaissance Computing Institute, the National
Center for Ecological Analysis and Synthesis, rOpenSci, DataONE, and
Mozilla Science Lab. M. Cantiello, E. Robinson, L. Winslow, J. Couture,
C. Granade, S. Earl, J. Ranganathan, and D. LeBauer provided valuable
ideas and feedback.

**Literature Cited**

Beaver, D. deB, and R. Rosen. 1978. Studies in scientific collaboration.
Scientometrics 1:65–84.

Byrnes, J. E. K., E. B. Baskerville, B. Caron, C. Neylon, C. Tenopir, M.
Schildhauer, A. Budden, L. Aarssen, and C. Lortie. 2014. The four
pillars of scholarly publishing: The future and a foundation. Ideas in
Ecology and Evolution 7.

Carpenter, S. R., E. V. Armbrust, P. W. Arzberger, F. S. Chapin, J. J.
Elser, E. J. Hackett, A. R. Ives, P. M. Kareiva, M. A. Leibold, P.
Lundberg, M. Mangel, N. Merchant, W. W. Murdoch, M. A. Palmer, D. P. C.
Peters, S. T. A. Pickett, K. K. Smith, D. H. Wall, and A. S. Zimmerman.
2009. Accelerate Synthesis in Ecology and Environmental Sciences.
BioScience 59:699–701.

Casadevall, A., and F. C. Fang. 2010. Reproducible Science. Infection
and Immunity 78:4972–4975.

Cerf, V. 2002. The Internet is for Everyone.
http://tools.ietf.org/html/rfc3271.

Chan, L., D. Cuplinskas, M. Eisen, F. Friend, Y. Genova, J.-C. Guédon,
M. Hagemann, S. Harnad, R. Johnson, R. Kupryte, M. La Manna, I. Rév, M.
Segbert, S. de Souza, P. Suber, and J. Velterop. 2002, February 14.
Budapest Open Access Initiative.
http://www.opensocietyfoundations.org/openaccess/read.

Collins, H. M. 2001. Tacit Knowledge, Trust and the Q of Sapphire.
Social Studies of Science 31:71–85.

Desjardins-Proulx, P., E. P. White, J. J. Adamson, K. Ram, T. Poisot,
and D. Gravel. 2013. The Case for Open Preprints in Biology. PLoS Biol
11:e1001563.

Dietze, M. C., D. S. Lebauer, and R. Kooper. 2013. On improving the
communication between models and data. Plant, Cell & Environment
36:1575–1585.

Drummond, D. C. 2009. Replicability is not Reproducibility: Nor is it
Good Science. Proceedings of the Evaluation Methods for Machine 
Learning Workshop 26th ICML, Montreal, Quebec, Canada. 
http://www.csi.uottawa.ca/∼cdrummon/pubs/ICMLws09.pdf.

Ellison, A. M. 2010. Repeatability and transparency in ecological
research. Ecology 91:2536–2539.

Emily S Darling, David Shiffman, Isabelle M. Côté, and Joshua A Drew.
2013. The role of Twitter in the life cycle of a scientific publication.
Ideas in Ecology and Evolution 6:32–43.

FitzJohn, R. G., M. W. Pennell, A. E. Zanne, P. F. Stevens, D. C. Tank,
and W. K. Cornwell. 2014. How much of the world is woody? Journal of
Ecology 102:1266–1272.

Franco, A., N. Malhotra, and G. Simonovits. 2014. Publication bias in
the social sciences: Unlocking the file drawer. Science:1255484.

Gacek, C., and B. Arief. 2004. The many meanings of open source. IEEE
Software 21:34–40.

Gaston, J. 1971. Secretiveness and competition for priority of discovery
in Physics. Minerva 9:472–492.

Gewin, V. 2013. Turning point: Carl Boettiger. Nature 493:711–711.

Hackett, E. J. 2005. Essential Tensions Identity, Control, and Risk in
Research. Social Studies of Science 35:787–826.

Hampton, S. E., C. A. Strasser, J. J. Tewksbury, W. K. Gram, A. E.
Budden, A. L. Batcheller, C. S. Duke, and J. H. Porter. 2013. Big data
and the future for ecology. Frontiers In Ecology And The Environment
11:156–162.

Hey, T., S. Tansley, and K. Tolle. 2009. The Fourth Paradigm:
Data-Intensive Scientific Discovery. 1 edition. Microsoft Research,
Redmond, Wash.

Iyengar, S., and J. B. Greenhouse. 1988. Selection Models and the File
Drawer Problem. Statistical Science 3:109–117.

Jasny, B. R., G. Chin, L. Chong, and S. Vignieri. 2011. Again, and
Again, and Again …. Science 334:1225–1225.

Lindenmayer, D., and G. E. Likens. 2013. Benchmarking Open Access
Science Against Good Science. Bulletin of the Ecological Society of
America 94:338–340.

Merton, R. K. 1957. Priorities in Scientific Discovery: A Chapter in the
Sociology of Science. American Sociological Review 22:635–659.

Michener, W. K., and M. B. Jones. 2012. Ecoinformatics: supporting
ecology as a data-intensive science. Trends in Ecology & Evolution
27:85–93.

Molloy, J. C. 2011. The Open Knowledge Foundation: Open Data Means
Better Science. PLoS Biol 9:e1001195.

Mosley, M., M. H. Brackett, S. Earley, and D. Henderson. 2009. DAMA
guide to the data management body of knowledge. Technics Publications.

National Science Foundation. 2007. NSF and the Birth of the Internet.
http://www.nsf.gov/news/special\_reports/nsf-net/.

Nielsen, M. 2011. Reinventing Discovery: The New Era of Networked
Science. Reprint edition. Princeton University Press.

Obama, B. 2013, May 9. Executive Order -- Making Open and Machine
Readable the New Default for Government Information. The White House.

Peng, R. D. 2009. Reproducible research and. Biostatistics 10:405–408.

Ram, K. 2013. Git can facilitate greater reproducibility and increased
transparency in science. Source Code for Biology and Medicine 8:7.

Reichman, O. J., M. B. Jones, and M. P. Schildhauer. 2011. Challenges
and Opportunities of Open Data in Ecology. Science 331:703 –705.

Sieber, J. E. 1989. Sharing Scientific Data I: New Problems for IRBs.
IRB: Ethics and Human Research 11:4.

Stodden, V. 2009. Enabling Reproducible Research: Open Licensing for
Scientific Innovation. SSRN Scholarly Paper, Social Science Research
Network, Rochester, NY.

Stodden, V., J. Borwein, and D. H. Bailey. 2013. “Setting the default to
reproducible” in computational science research. SIAM News 46:4–6.

Strasser, C. A., and S. E. Hampton. 2012. The fractured lab notebook:
undergraduates and ecological data management training in the United
States. Ecosphere 3:art116.

Wagner, C. S. 2008. The New Invisible College: Science for Development.
Brookings Institution Press.

Wilson, G., D. Aruliah, C. T. Brown, N. P. C. Hong, M. Davis, R. T. Guy,
S. H. Haddock, K. D. Huff, I. M. Mitchell, M. D. Plumbley, and others.
2014. Best practices for scientific computing. PLoS biology 12:e1001745.

Wuchty, S., B. F. Jones, and B. Uzzi. 2007. The Increasing Dominance of
Teams in Production of Knowledge. Science 316:1036–1039.

**Table 1:** A wide range of tools is available to support open science
at each stage of the research life-cycle.

  **Concept**                                         **Name of Tool or Service**                              **Tool Description**
  --------------------------------------------------- -------------------------------------------------------- -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  ***Ideas & communication***                                                                                  
  Open discussion                                     Twitter                                                  Twitter allows users to write, share, and respond to short 140 character messages. An ever-increasing community of scientists uses Twitter to share ideas about research (Darling et al. 2013)
                                                      Blogs                                                    Blogs can be hosted on university websites, personal servers or blogging sites (e.g. wordpress.com). Blogs offer an informal means of discussing ideas, results, published literature etc.
                                                      Open lab notebook                                        Open lab notebooks apply the concept of blogging to day-to-day research work: research notes and data are published online as they are accumulated.
                                                      GitHub comments                                          GitHub comments allow others to review code and offer comments on particular sections.
  ***Hypotheses/ Design***                                                                                     
                                                      Data Management Planning Tool                            The Data Management Planning Tool enables researchers to easily create, manage and share data management plans that meet the requirements of a broad array of funding agencies and institutions.
  ***Data collection***                                                                                        
  Support of the Data Life-cycle                      Data repositories: KNB, DataONE, Dryad                   Data repositories make data available to future researchers and allow the reproducibility of research; they are a cornerstone of open science.
                                                      Open Office                                              Open Office is a comprehensive office tool suite that supports word processing, spreadsheets, graphics, presentations, drawing, and creating and maintaining databases.
                                                      MySQL                                                    mySQL is a popular and widely used open-source relational database management system (RDBMS) based on Structured Query Language (SQL).
                                                      Open Refine                                              OpenRefine is a powerful tool for exploring and cleaning messy data, transforming it from one format into another, and link and extending data with web services and databases.
                                                      Morpho                                                   Morpho is a program that can be used to enter metadata, which are stored in a file that conforms to the Ecological Metadata Language (EML) specification.
  ***Analyze and Visualize***                                                                                  
  Reproducibility                                     R                                                        R is a widely used statistical programming language that is commonly used for analyzing and visualizing data
                                                      RStudio                                                  RStudio is a Integrated Development Environment (IDE) for R
                                                      Python                                                   Python is a widely used high level programming language that is commonly used for managing and manipulating data.
                                                      Pycharm                                                  Pycharm is one of several IDEs available for python
  Version control                                     Git and GitHub                                           Git is a piece of software that allows you to create 'versions' of your code, text, and project files as you work on them. GitHub is a website that allows this to be done collaboratively, with social and discussion features built in.
  Free alternatives for GIS                           GRASS                                                    GRASS (Geographic Resources Analysis Support System), is a Geographic Information System (GIS) software toolset used for geospatial data management, analysis, and visualization, as well as image processing and spatial modeling.
                                                      QGIS                                                     QGIS is a desktop GIS application that supports geospatial data viewing, editing, and analysis.
  Workflow tools                                      Kepler                                                   Kepler is a scientific workflow package that allows researchers to create, execute, and share analytical models.
                                                      VisTrails                                                VisTrails is a scientific workflow and provenance management system that supports data exploration and visualization.
  Reproducible documents                              Sweave                                                   Sweave was originally a way to integrate S and LaTeX, but now also works with R.
                                                      IPython notebook / Project Jupyter                       The IPython notebook (now renamed Project Jupyter and focusing on R and Julia in addition to python) is a tool for interactively analyzing and processing data in the browser using blocks of code.
                                                      markdown                                                 Markdown is a simple markup syntax for adding formatting to documents. It allows correctly formatted scientific documents to be written in plain text.
                                                      pandoc                                                   Pandoc allows conversion between many document types, including LaTeX, markdown, PDF, and Word (.docx)
                                                      knitr                                                    knitr is a newer package that allows the integration of a whole number of different scripting languages in a single document
                                                      Rmarkdown                                                Rmarkdown is an authoring format which combines markdown with the syntax of both knitr and pandoc
  ***Presenting preliminary results***                                                                         
  Share results and get feedback                      Figshare                                                 An online repository for all types of research products (data, posters, slides, etc) that assigns each a citable DOI
                                                      Slideshare                                               An online clearinghouse for presentation slides of all types
                                                      Speakerdeck                                              An online site for sharing PDF presentations (run by GitHub)
  ***Writing***                                                                                                
  Enhance collaboration                               Google Docs                                              Online collaborative writing, spreadsheet, and presentations tools
                                                      Etherpad                                                 Online, open source, collaborative writing tool
                                                      ShareLateX                                               Online collaborative writing tool (like Google docs) focused on LaTeX
                                                      WriteLaTeX                                               Online collaborative writing tool (like Google docs) focused on LaTeX
                                                      Authorea                                                 Online collaborative writing tool (like Google docs) focused on LaTeX
  Citing research                                     Zotero                                                   Zotero is a free and open source extension to the Firefox browser (and now a standalone app) that can help with literature management and citation
                                                      Mendeley                                                 Mendeley is a free reference manager and social network for researchers.
  ***Preprint***                                                                                               
  Share results and get feedback                      bioRXiv                                                  bioRXiv, run by Cold Spring Harbor, is a relatively new preprint server that focuses more exclusively on biology
                                                      arXiv                                                    arXiv is one of the original preprint server on the web. Run by Cornell, it is mainly focused on math, physics, and computer science, although it has been used by quantitative biologists as well.
                                                      PeerJ Preprints                                          PeerJ Preprints is a preprint server run by the open-access online-only journal PeerJ
  ***Pre-publication peer preview***                                                                           
                                                      Peerage of Science                                       Pre-publication formal peer review (and review of the reviews), which can then be sent on to participating journals
                                                      Axios Review                                             Pre-publication formal peer review and appraisal of a manuscript's fit with targetted journals; reviews can then be sent on to participating journals
  ***Publish***                                                                                                
  Reduce barriers to access                           DOI for code                                             Code can be given a DOI and cited in the literature. For example, a Github repository can be assinged a DOI via zenodo.org
                                                      DOI for data                                             Data uploaded to any of the numerous available online repositories will be assigned a DOI and is then citeable by other researchers using that dataset
                                                      "Green" open access                                      Open access whereby the author of a manuscript may post a pdf of their article to their own personal website
                                                      "Gold" open access                                       Open access where the authors pay a fee up-front (before publication) to allow their paper to be made open
                                                      Licences: CC-BY, CC-BY-NC etc                            Licenses dicatate how a research product may be used by others - some have restrictions necessitating attribution, others mandate no commercial reuse, etc
  ***Discussion of published literature and data***                                                            
  Finding published data                              DataONE                                                  DataONE is a federation of data repositories that supports easy discovery of and access to environmental and Earth science data, as well as various data management tools and educational resources.
                                                      re3data                                                  re3data is a registry of digital repositories that enables researchers to discover public and institutional repositories where they may deposit and preserve their data.
  Social networking for academics                     ResearchGate                                             A social networking and question and answer site for academics
                                                      Academia.edu                                             Social netowrk for academics
  Tracking research product impact                    ORCID                                                    Unique identifiers for individual researchers, which allows contributions to be tracked across many repositories, grant proposals, peer review sites, etc.
                                                      ImpactStory                                              ImpactStory can track almost all of the research contributions (data, code and papers) by individual researchers, and quantifies their impacts using open data sources (e.g. tweets, use in wikipedia articles, saves in Mendeley)
                                                      Altmetric                                                Provides metrics (tweets, blog posts, Mendeley saves, etc) of individual research objects
  Informal discussion                                 Conference or hallway conversations, discussion groups   High efficiency but limited accessibility to outside researchers
                                                      Personal website/blog                                    Personal blogs can be a forum to discuss both one's own research as well as the research of other scientists

------

**Box 1:  **Technological advances driven by scientists

Every scientist now uses the Internet, but few are aware of how the
Internet grew out of a highly collaborative and open process involving
development of publicly available and commentable standard protocols
which in turn spawned a vast, highly interoperable, international,
communications network (\<http://www.fcc.gov/openinternet\>; Cerf 2002).
First coined in the 1990s, the term "open source" encompasses not only
compilers and applications but also protocols and specifications such as
the domain name system (DNS) that allows pinpointing specific networked
computers ("hosts") around the world, and HTTP/HTML specifications that
provide the basis for the World Wide Web. The availability of open
source software radically democratized and expanded participation in the
Internet community in the late 1980s-early 1990s.

Members of the scientific research community were early recipients of
these advantages, with the National Science Foundation supporting and
nurturing growth of the Internet-based NSFNET from roughly 1985-1995
(National Science Foundation, 2007). In that era, it was scientists who
were largely communicating through the Internet (gopher, email),
transferring their data (FTP), and running analyses on remote servers
(telnet, shell access, X11), often with privileged access to fast
networks and accounts on powerful computational servers. Within this
computer savvy community, "power users" leveraged the Internet most
effectively via learning computational skills that were largely
command-line based. The legendary, free GNU suite of software was
standard issue for many computers joining the Internet in the late
1980s, and made that early generation of networked "scientific
workstations" (from Sun, SGI, DEC, or NeXT) the sought-after systems of
their day.

These early forays into powerful software helped birth the plethora of
tools now available to the modern scientist.  Today, free,
multi-platform, open source tools from the Linux Foundation (free
operating system), the Apache Software Foundation (free Web server), the
Mozilla Foundation (free Web, email, and other applications), the
PostgreSQL Global Development Group (free enterprise database), the
Python Software Foundation (free programming language), and the R
Foundation for Statistical Computing (analysis and statistical language)
are enabling researchers across the globe to dialog with one another via
cutting edge communication, execute powerful data manipulation, and
develop community-vetted modeling and analysis tools at little or no
cost.

------------------------

**Box 2:** “A glossary of open science for ecologists”

***citizen science:  ***enabling interested citizens to contribute their
time, observations, and expertise such that these assist and inform the
scientific research process; may be an aspect of ***crowd-sourcing***.

***code repository***: an accessible, central place where computer code
is stored to facilitate the collection, manipulation, analysis, or
display of data.

***crowd-sourcing***: leveraging the expertise and participation of many
individuals, to provide more perspectives, critiques, data
contributions, code contributions, etc. to advance a (scientific)
process.

***data life-cycle***: the pathway researchers trace when documenting
the natural world from idea generation through to making observations
and drawing inference. It is popularly dissected into eight intergrading
phases: Plan, Collect, Assure, Describe, Preserve, Discover, Integrate,
Analyze (Michener et al 2012).

***data management***: the development and execution of architectures,
policies, practices and procedures that properly manage the full ***data
life-cycle*** needs of an enterprise (Mosley et al. 2009).

***data repository***: an accessible, central place where accumulated
files containing collected information are permanently stored; typically
these house multiple sets of databases and/or files.

***open access***: providing free and unrestricted access research
products, especially journal articles and white papers-- to be read,
downloaded, distributed, reanalyzed, or used for any other legal
purpose, while affording authors control over the integrity of their
work and the right to be acknowledged and cited. (adapted from the
Budapest Open Access Initiative definition, Chan et al. 2002).

***open data***: data that can be freely used, reused, and redistributed
without restrictions beyond a requirement for attribution and
share-alike (Molloy 2011).

***open science***: Open Science is the idea that scientific knowledge,
including data, observational and experimental design and methods,
analytical and modeling code, as well as results and interpretations of
these (e.g. reported in publications)-- are made freely accessible to
anyone, and represented in transparent and reusable formats as early as
practical in the discovery process by employing standards-based
technology tools.  ***open science*** frequently encompasses all of
***open access***, ***open data*** and ***open source*** and, minimally,
facilitates reproducibility of results.

***open source:*** computer code (software) that is available for free
distribution and re-use, with source code unobscured, and explicit
acknowledgement of the right to create derived works by modifying the
code (Gacek and Arief 2004)

***preprint***: a draft version of a paper distributed (usually in an
online repository such as arXiv) before a final, peer-reviewed journal
or reporting agency has accepted or formally published the paper
(Desjardins-Proulx et al, 2013).

***reproducibility, replicability, and repeatability***: while formal
definitions of these terms vary widely and across disciplines, these all
point to a hallmark of science, which is the ability to repeatedly
generate or observe outcomes consistent with scientific understanding,
based on explicit specification of theories, models, and methods, and
their expected material realizations or outcomes.  This prescribes a
need for sufficient access to data and analytical code to verify that a
purported result is valid, as well as to examine these for errors and
biases (Jasny et al 2011; Peng 2009; Stodden et al. 2013; Stodden 2009;
but note Drummond 2009 and Casadevall and Fang 2010 use somewhat
different definitions)      

***transparency***: a scientific process is  described in enough detail
that it is open to public scrutiny and examination; nothing is
intentionally obscured by technology or process  ***         ***

***version control***: a system that manages snapshots (and hence
“revisions” or “versioning”) of code and data for a project (Wilson et
al 2014). It facilitates detailed documentation to enable tracing any
significant changes over a project’s lifetime.

       

----------------------------------------------------

**Figure 1.** Increase in “open science” in the literature

**Figure 2:** Three examples of possible open science workflows. In each
workflow illustration, the *gray box* surrounds activities that are not
openly accessible for researchers who are not directly involved.
Activities outside these boxes are open and available, representing how
the individual researcher is influenced by other scholars, or is able to
communicate their research before and after publication. *White boxes*
represent distinct research products available for reference for and
feedback from other researchers.

![](media/image1.png)
